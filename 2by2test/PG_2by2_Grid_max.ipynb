{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csrl.mdp import GridMDP\n",
    "from csrl.oa import OmegaAutomaton\n",
    "from csrl import ControlSynthesis\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# import scipy.special as sc\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using dill to import product MDP generated offline, avoiding installation of rabinzer4 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "dill.load_session('2by2.pkl')\n",
    "# dill.load_session('product_MDP_2_Gammma_0.9_0.99.pkl')\n",
    "print(csrl.discount)\n",
    "print(csrl.discountB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_state(theta,state,csrl):\n",
    "    Action_set = csrl.A[state]  \n",
    "    # temp = theta[state][:len(Action_set)] / np.linalg.norm(theta[state][:len(Action_set)])\n",
    "    temp = theta[state][:len(Action_set)]\n",
    "    # Policy_A_theta = softmax(temp)\n",
    "    Policy_A_theta = np.exp(temp) / np.sum(np.exp(temp))\n",
    "    return Policy_A_theta\n",
    "\n",
    "def derivate_softmax(theta,state,csrl,action,Policy_A_theta):\n",
    "    Action_set = csrl.A[state]\n",
    "    P_a = Policy_A_theta[Action_set.index(action)]\n",
    "    Grad_Pi = np.zeros([len(Action_set),1])\n",
    "    for i in range(len(Grad_Pi)):\n",
    "        if i == Action_set.index(action):\n",
    "            Grad_Pi[i] = P_a*(1-P_a)\n",
    "        else:\n",
    "            Grad_Pi[i] = -P_a * Policy_A_theta[i]\n",
    "    return Grad_Pi\n",
    "\n",
    "# @jit(nopython=False)\n",
    "def one_trajectroy(initial_state,csrl,theta,length): \n",
    "    state   = initial_state\n",
    "    reward  = csrl.reward[state]\n",
    "    state_hist  = [state]\n",
    "    reward_hist = [reward]\n",
    "    gamma_hist  = [csrl.discountB if reward else csrl.discount]\n",
    "    action_hist = []\n",
    "    for t in range(length):\n",
    "        A_set = csrl.A[state]\n",
    "        # softmax, from theta to probability\n",
    "        # Policy, action distributation given state and theta\n",
    "        \n",
    "#         action = A_set[np.random.choice(len(A_set),p= softmax_state(theta,state,csrl) )]\n",
    "        \n",
    "\n",
    "        action = A_set[np.argmax(softmax_state(theta,state,csrl) )]\n",
    "        \n",
    "        states, probs = csrl.transition_probs[state][action]\n",
    "        state = states[np.random.choice(len(states),p=probs)]\n",
    "        reward = csrl.reward[state]\n",
    "        gamma = csrl.discountB if reward else csrl.discount\n",
    "        state_hist.append(state)\n",
    "        reward_hist.append(reward)\n",
    "        gamma_hist.append(gamma)\n",
    "        action_hist.append(action)\n",
    "    state_hist  = state_hist[:-1]\n",
    "    reward_hist = reward_hist[:-1]\n",
    "    gamma_hist  = gamma_hist[:-1]\n",
    "    G_t = reward_hist[-1]\n",
    "    G_t_hist = [G_t]\n",
    "    for t in range(length-2,-1,-1):\n",
    "        G_t = reward_hist[t] + gamma_hist[t] * G_t\n",
    "        G_t_hist.append(G_t)\n",
    "    G_t_hist.reverse()\n",
    "    return state_hist, action_hist, G_t_hist, gamma_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this derivate_softmax is $\\frac{\\partial \\pi(a_t|s)}{\\partial \\theta} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999\n",
      "0.99\n",
      "[[[[0.   0.  ]\n",
      "   [0.   0.  ]]\n",
      "\n",
      "  [[0.   0.  ]\n",
      "   [0.   0.01]]\n",
      "\n",
      "  [[0.   0.  ]\n",
      "   [0.   0.  ]]]]\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "T = 100\n",
    "K = 10000\n",
    "np.random.seed(2)\n",
    "\n",
    "theta = np.zeros(csrl.shape)\n",
    "theta = np.random.random(csrl.shape)\n",
    "theta_hist = np.zeros( [K+1] + list(csrl.shape) )\n",
    "theta_hist[0] = theta\n",
    "theta_hist = np.zeros( [K+1] + list(csrl.shape) )\n",
    "V = np.zeros(csrl.shape[:-1])\n",
    "V_hist = np.zeros([K+1] + list(csrl.shape[:-1]) )\n",
    "\n",
    "print(csrl.discount)\n",
    "print(csrl.discountB)\n",
    "print(csrl.reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$T$ batch size, $K$ steps, set $\\gamma>\\gamma_B$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8105/10000 [01:08<00:15, 119.24it/s]"
     ]
    }
   ],
   "source": [
    "for k in tqdm(range(K)):\n",
    "    # k_th episode\n",
    "# =============================================================================\n",
    "# simulate and record a trajectroy,\n",
    "    # initial state    \n",
    "    state   = (csrl.shape[0]-1,csrl.oa.q0)+(csrl.mdp.random_state())  \n",
    "    state_hist, action_hist, G_t_hist, gamma_hist = one_trajectroy(state,csrl,theta,T+200)\n",
    "# =============================================================================\n",
    "# use 1 trajectory to generate policy gradient and update parameter once, \n",
    "    # alpha = np.max((1.0*(1 - 1.5*k/K),0.001))\n",
    "    alpha = 0.05\n",
    "\n",
    "    for t3 in range(T):\n",
    "\n",
    "        state   = state_hist[t3]\n",
    "        if not (state in state_hist[0:t3]) :            \n",
    "            action  = action_hist[t3]\n",
    "            A_set   = csrl.A[state]\n",
    "            # softmax, from theta to probability\n",
    "            Policy_s = softmax_state(theta,state,csrl)\n",
    "            P_a = Policy_s[A_set.index(action)]\n",
    "            # derivate of log softmax, pi w.r.t. theta\n",
    "            Grad_Pi = derivate_softmax(theta,state,csrl,action,Policy_s) / P_a\n",
    "            \n",
    "            PG = np.zeros(theta.shape)\n",
    "            Grad_V = np.zeros(V.shape)\n",
    "            \n",
    "            # PG_state = np.prod(gamma_hist[0:t3:1])* (G_t_hist[t3] + np.prod(gamma_hist[t3:-1:1]) * V[state_hist[-1]]- V[state]) * Grad_Pi \n",
    "            PG_state = np.prod(gamma_hist[0:t3:1])* (G_t_hist[t3]-V[state_hist[t3]]) * Grad_Pi \n",
    "            PG[state][0:len(PG_state):1] += PG_state.flatten()\n",
    "            Grad_V[state] = G_t_hist[t3] - V[state]    \n",
    "            \n",
    "            theta = theta + 0.1*PG\n",
    "            V     = V     + alpha * Grad_V\n",
    "        \n",
    "        # PG_state = np.prod(gamma_hist[0:t3:1]) * (G_t_hist[t3] - V[state]) * Grad_Pi / P_a\n",
    "        \n",
    "    #     PG_state = np.prod(gamma_hist[0:t3:1]) * (G_t_hist[t3] + V[state_hist[-1]] - V[state]) * Grad_Pi \n",
    "    #     PG[state][0:len(PG_state):1] += PG_state.flatten()\n",
    "    #     Grad_V[state] = Grad_V[state] + G_t_hist[t3] - V[state]       \n",
    "    # theta = theta   + PG / T\n",
    "    # V     = V       + Grad_V/T\n",
    "    \n",
    "    V_hist[k+1] = V\n",
    "    theta_hist[k+1] = theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose a random initial state, generate 1 trajectory $\\tau$, use this trajectory to update $\\theta$ once,\n",
    "$ \\theta = \\theta +  \\mathbb{E}_\\tau \\big{[} \\Gamma(0:t)[G_t + \\hat{V}(s_T) - \\hat{V}(s_t)]  \\nabla_{\\theta} \\log (\\pi_{\\theta}(a_t|s_t)\\big{]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% from theta to policy\n",
    "policy = np.zeros(csrl.shape[:-1])\n",
    "for i,q,r,c in csrl.states():\n",
    "    state = (i,q,r,c)\n",
    "    # temp = theta[state][0:len(csrl.A[state])]\n",
    "    temp = theta[state][csrl.A[state]]\n",
    "    policy[state] = csrl.A[state][np.argmax(temp)]\n",
    "    policy = policy.astype(int)\n",
    "print('policy')\n",
    "print(policy[0,0])\n",
    "print('value')\n",
    "print( np.around( (np.sum(V_hist[round(k/10*9):k:1],axis=0)/(k/10))[0,0],decimals = 2 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use PR-averging, take avergae of a portition tail as result, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "plt.figure()\n",
    "# plt.hold(True)\n",
    "plt.plot(range(k),V_hist[0:k:1,0,0,0,0])\n",
    "plt.plot(range(k),V_hist[0:k:1,0,0,0,1])\n",
    "plt.plot(range(k),V_hist[0:k:1,0,0,1,0])\n",
    "plt.plot(range(k),V_hist[0:k:1,0,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "plt.figure()\n",
    "plt.plot(range(k),theta_hist[0:k:1,0,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csrl.mdp.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
